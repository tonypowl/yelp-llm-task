{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a7ace42-0200-451e-808b-c550c245b9db",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "667d7037-7cf9-4d9b-b00f-eb273bb29e55",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\n",
    "    \"yelp.csv\",\n",
    "    usecols=[\"text\", \"stars\"]\n",
    ")\n",
    "df_eval = df.sample(200, random_state=42)\n",
    "print(df.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae9512ed-f700-46a7-91cd-15864422c157",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import google.generativeai as genai\n",
    "\n",
    "load_dotenv()\n",
    "genai.configure(api_key=os.getenv(\"GEMINI_API_KEY\"))\n",
    "\n",
    "model = genai.GenerativeModel(\"models/gemini-flash-latest\")\n",
    "\n",
    "response = model.generate_content(\"This is just a check to see if the model was called correctly, mention which specific model this is and say model called succeffuly if it works\")\n",
    "print(response.text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab0f394e-0787-4ea8-80b8-4a0173e9407a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#PROMPT 1 - simple text straight from the assignment with an example \n",
    "def prompt1(review):\n",
    "    return f\"\"\"\n",
    "    Classify the following Yelp review into a star rating from 1 to 5.\n",
    "    return only a valid json format answer in the exact following way without adding anything:\n",
    "    {{\n",
    "      \"predicted_stars\": 4,\n",
    "      \"explanation\": \"Brief reasoning for the assigned rating.\"\n",
    "    }}\n",
    "    \n",
    "    Review:\n",
    "    \"{review}\"\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f363013d-af9d-42f6-b3b8-e3da6165f71f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#PROMPT 2 - a more complex prompt allowing the model to think with an example to allow for even more intuitive answers\n",
    "\n",
    "def prompt2(review):\n",
    "    return f\"\"\"\n",
    "    You are given a Yelp review, \n",
    "    the first step is to analyze the overall sentiment and customer experience described in the review.\n",
    "    Then, assign a star rating from 1 to 5 based on the sentiment strength where 1 is a very negative experience \n",
    "    and 5 is a very positive experience.\n",
    "    \n",
    "    Return only valid json in the exact format below without adding any extra content or markdown text:\n",
    "    \n",
    "    {{\n",
    "      \"predicted_stars\": <1-5>,\n",
    "      \"explanation\": \"Brief reasoning based on sentiment in the review\"\n",
    "    }}\n",
    "      Review:\n",
    "      \"{review}\"\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "457b6f46-4a78-484b-b866-668686608898",
   "metadata": {},
   "outputs": [],
   "source": [
    "#PROMPT 3 - an even more complex prompt including multiple real exampls from the dataset \n",
    "def prompt3(review):\n",
    "    return f\"\"\"\n",
    "    Classify the following Yelp review into a star rating from 1 to 5.\n",
    "    return only a valid json format answer in the exact following way without adding anything in the specified format after going \n",
    "    through other real Yelp reviews given by people. \n",
    "    \n",
    "    Below are examples taken from real Yelp reviews with their correct ratings.\n",
    "    \n",
    "    Ex 1:\n",
    "    Review:\n",
    "    \"My wife took me here on my birthday for breakfast and it was excellent. The weather was perfect which made sitting outside overlooking their grounds an absolute pleasure. Our waitress was excellent and our food arrived quickly...\"\n",
    "     Output:\n",
    "    {{\n",
    "      \"predicted_stars\": 5,\n",
    "      \"explanation\": \"The review describes an excellent experience with great service, food, and atmosphere, with no negative feedback.\"\n",
    "    }}\n",
    "    \n",
    "    Ex 2:\n",
    "    Review:\n",
    "    \"I have no idea why some people give bad reviews about this place. Everyone was very pleasant, the food was awesome, and the prices were very good.\"\n",
    "     Output:\n",
    "    {{\n",
    "      \"predicted_stars\": 5,\n",
    "      \"explanation\": \"The review strongly praises the service, food quality, and overall experience.\"\n",
    "    }}\n",
    "    \n",
    "    Ex 3:\n",
    "    Review:\n",
    "    \"Love the gyro plate. Rice is so good and I also dig their candy selection :)\"\n",
    "     Output:\n",
    "    {{\n",
    "      \"predicted_stars\": 4,\n",
    "      \"explanation\": \"The review is positive but brief, expressing satisfaction without strong enthusiasm.\"\n",
    "    }}\n",
    "    Don't include markdown text or extra commentary. \n",
    "      Review:\n",
    "    \"{review}\"\n",
    "    \"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67c37c80-f79f-4c7f-aca0-f7f1dcfb7666",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_p1 = []\n",
    "for _, row in df_eval.iterrows():\n",
    "    response = model.generate_content(prompt1(row[\"text\"]))\n",
    "    raw = response.text.strip()\n",
    "    try:\n",
    "        parsed = json.loads(raw)\n",
    "        \n",
    "        results_p1.append({\n",
    "            \"actual\": row[\"stars\"],\n",
    "            \"predicted\": parsed[\"predicted_stars\"],\n",
    "            \"json_valid\": True\n",
    "        })\n",
    "    except Exception:\n",
    "        results_p1.append({\n",
    "            \"actual\": row[\"stars\"],\n",
    "            \"predicted\": None,\n",
    "            \"json_valid\": False\n",
    "        })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b6afc61-0408-4cea-beea-5ac1b2505060",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_p2 = []\n",
    "for _, row in df_eval.iterrows():\n",
    "    response = model.generate_content(prompt2(row[\"text\"]))\n",
    "    raw = response.text.strip()\n",
    "    try:\n",
    "        parsed = json.loads(raw)\n",
    "\n",
    "        results_p2.append({\n",
    "            \"actual\": row[\"stars\"],\n",
    "            \"predicted\": parsed[\"predicted_stars\"],\n",
    "            \"json_valid\": True\n",
    "        })\n",
    "    except Exception:\n",
    "        results_p2.append({\n",
    "            \"actual\": row[\"stars\"],\n",
    "            \"predicted\": None,\n",
    "            \"json_valid\": False\n",
    "        })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ff5d6ad-ec77-4614-a31c-8d87198ac835",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_p3 = []\n",
    "for _, row in df_eval.iterrows():\n",
    "    response = model.generate_content(prompt3(row[\"text\"]))\n",
    "    raw = response.text.strip()\n",
    "    try:\n",
    "        parsed = json.loads(raw)\n",
    "\n",
    "        results_p3.append({\n",
    "            \"actual\": row[\"stars\"],\n",
    "            \"predicted\": parsed[\"predicted_stars\"],\n",
    "            \"json_valid\": True\n",
    "        })\n",
    "    except Exception:\n",
    "        results_p3.append({\n",
    "            \"actual\": row[\"stars\"],\n",
    "            \"predicted\": None,\n",
    "            \"json_valid\": False\n",
    "        })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5a0d384-a349-4129-af4a-a63a3f1e0243",
   "metadata": {},
   "outputs": [],
   "source": [
    "#converting them to dataframes\n",
    "df_p1 = pd.DataFrame(results_p1)\n",
    "df_p2 = pd.DataFrame(results_p2)\n",
    "df_p3 = pd.DataFrame(results_p3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec0edc02-eeb7-49f9-b73d-925edb6bd6c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#accuracy\n",
    "acc_p1 = (df_p1.dropna()[\"actual\"] == df_p1.dropna()[\"predicted\"]).mean()\n",
    "acc_p2 = (df_p2.dropna()[\"actual\"] == df_p2.dropna()[\"predicted\"]).mean()\n",
    "acc_p3 = (df_p3.dropna()[\"actual\"] == df_p3.dropna()[\"predicted\"]).mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8bd3d21-79e3-43f8-91c1-5fce921a135b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#json validity rate\n",
    "json_rate_p1 = df_p1[\"json_valid\"].mean()\n",
    "json_rate_p2 = df_p2[\"json_valid\"].mean()\n",
    "json_rate_p3 = df_p3[\"json_valid\"].mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5d5dd09",
   "metadata": {},
   "outputs": [],
   "source": [
    "#consistency (basically standard deviation)\n",
    "std_p1 = df_p1[\"predicted\"].std()\n",
    "std_p2 = df_p2[\"predicted\"].std()\n",
    "std_p3 = df_p3[\"predicted\"].std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e28136d8-a6be-4b1a-bffa-124464aa8b7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#comparision table\n",
    "comparison = pd.DataFrame({\n",
    "    \"Accuracy\": [acc_p1, acc_p2, acc_p3],\n",
    "    \"JSON Validity Rate\": [json_rate_p1, json_rate_p2, json_rate_p3],\n",
    "    \"Prediction Std (Consistency)\": [std_p1, std_p2, std_p3]\n",
    "}, index=[\"Prompt 1\", \"Prompt 2\", \"Prompt 3\"])\n",
    "\n",
    "comparison\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2da2978",
   "metadata": {},
   "source": [
    "Short Dicussion:\n",
    "This task was implemented using a local Jupyter Notebook. The yelp dataset was downloaded locally, and only the relevant text and stars columns were used. I chose Gemini api due to past usage and after encountering model issues with an older configuration, gemini-flash-latest was selected using the ListModel() funtion. I implemented and evaluated three different prompts on a subset of the data. Due to free-tier API rate limits, further large-scale testing was restricted after the request quota was over."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
